{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]   # key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4874a0-06ff-4ac2-a58b-5656d6799c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"stream\": FalseëŠ” OpenAI APIì—ì„œ ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°í• ì§€ ì—¬ë¶€ë¥¼ ì§€ì •í•˜ëŠ” ì˜µì…˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "âœ… ì˜ë¯¸ ì •ë¦¬\n",
    "stream=False:\n",
    "â†’ ì‘ë‹µì„ í•œ ë²ˆì— ëª¨ë‘ ë°›ì•„ì˜¤ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "â†’ ëª¨ë¸ì´ ì‘ë‹µì„ ë‹¤ ìƒì„±í•œ í›„ì— ì „ì²´ ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "stream=True:\n",
    "â†’ ì‘ë‹µì„ ì¡°ê°(stream) ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°›ì•„ì˜¤ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "â†’ ì˜ˆë¥¼ ë“¤ì–´, í† í° ë‹¨ìœ„ë¡œ ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ëŒ€ë¡œ ë°”ë¡œë°”ë¡œ ë°˜í™˜ë°›ì•„ ì‹¤ì‹œê°„ ì¶œë ¥ì²˜ëŸ¼ ë³´ì´ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ§  ì–¸ì œ ì‚¬ìš©í•˜ë‚˜?\n",
    "stream=False (ê¸°ë³¸ê°’):\n",
    "â†’ ì‘ë‹µ í¬ê¸°ê°€ ì‘ê³ , ì „ì²´ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ë° ë¬¸ì œê°€ ì—†ëŠ” ê²½ìš° ì í•©í•©ë‹ˆë‹¤.\n",
    "\n",
    "stream=True:\n",
    "â†’ ì‘ë‹µì´ ê¸¸ê±°ë‚˜ ëŒ€í™”í˜• UX(ì˜ˆ: ì±„íŒ… ì•± ë“±)ì—ì„œ ì‚¬ìš©ìê°€ ë‹µë³€ì„ ì ì  ì¶œë ¥ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³´ê²Œ í•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ“Œ ì˜ˆì‹œ ì°¨ì´\n",
    "stream=False\n",
    "json\n",
    "ì½”ë“œ ë³µì‚¬\n",
    "{\n",
    "  \"choices\": [\n",
    "    {\n",
    "      \"message\": {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"ì•ˆë…•í•˜ì„¸ìš”, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "stream=True\n",
    "json\n",
    "ì½”ë“œ ë³µì‚¬\n",
    "data: { \"choices\": [{ \"delta\": { \"role\": \"assistant\" }}]}\n",
    "data: { \"choices\": [{ \"delta\": { \"content\": \"ì•ˆ\" }}]}\n",
    "data: { \"choices\": [{ \"delta\": { \"content\": \"ë…•\" }}]}\n",
    "data: { \"choices\": [{ \"delta\": { \"content\": \"í•˜\" }}]}\n",
    "...\n",
    "ì´ì²˜ëŸ¼ stream=TrueëŠ” JSON ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë°ì´í„°ë¥¼ í† í° ë‹¨ìœ„ë¡œ í˜ë ¤ì¤ë‹ˆë‹¤.\n",
    "\n",
    "í•„ìš”í•˜ë‹¤ë©´ Python ì½”ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ëŠ” ë°©ì‹ë„ ì„¤ëª…í•´ ë“œë¦´ ìˆ˜ ìˆì–´ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ‰ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì Ÿ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡Š \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡ƒ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ² \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ³ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ƒ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ‰ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì Ÿ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡Š \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡ƒ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ² \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ³ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ƒ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 6.0 KB                         \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼…   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼…  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8b8148-68aa-46be-9152-e43a52367f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "í•´ë‹¹ ì½”ë“œëŠ” Ollamaë¼ëŠ” ë¡œì»¬ LLM ì‹¤í–‰ íˆ´ì„ ì‚¬ìš©í•  ë•Œ, llama3 ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œ(pull)í•˜ê¸° ìœ„í•œ ëª…ë ¹ì–´ì…ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ” ì½”ë“œ í•´ì„\n",
    "python\n",
    "ì½”ë“œ ë³µì‚¬\n",
    "# Let's just make sure the model is loaded\n",
    "!ollama pull llama3.2\n",
    "! :\n",
    "â†’ Jupyter Notebookì´ë‚˜ Google Colab ë“±ì—ì„œ ì…¸ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì ‘ë‘ì‚¬ì…ë‹ˆë‹¤.\n",
    "â†’ ì¼ë°˜ Python ì½”ë“œê°€ ì•„ë‹ˆë¼, í„°ë¯¸ë„ ëª…ë ¹ì–´ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "ollama pull llama3.2:\n",
    "â†’ ollama CLIë¥¼ ì‚¬ìš©í•´ llama3.2ë¼ëŠ” ëª¨ë¸ì„ ë¡œì»¬ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "â†’ ëª¨ë¸ì„ ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜¤ê³ , ì‹¤í–‰ ê°€ëŠ¥í•œ ìƒíƒœë¡œ ì¤€ë¹„í•´ë‘ê¸° ìœ„í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ§  ìš”ì•½í•˜ë©´\n",
    "ì´ ì½”ë“œëŠ” \"llama3.2\" ëª¨ë¸ì„ Ollamaë¥¼ í†µí•´ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œí•´ì„œ, ë‚˜ì¤‘ì— inference(ì§ˆë¬¸ ì‘ë‹µ ë“±)ì— ì‚¬ìš©í•  ì¤€ë¹„ë¥¼ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI (Artificial Intelligence) is a type of machine learning model that can generate new data, images, or text based on patterns learned from existing data. The business applications of Generative AI are diverse and continue to grow as technology advances. Here are some examples:\n",
      "\n",
      "1. **Content Generation**: Generative AI can be used to generate high-quality content such as:\n",
      " * Product descriptions\n",
      " * Social media posts\n",
      " * Blog articles\n",
      " * Email newsletters\n",
      " * Videos (e.g., explainer videos, product demos)\n",
      "2. **Image and Video Editing**: Generative AI can be used for image and video editing tasks, such as:\n",
      " * Image manipulation (e.g., object removal, super-resolution)\n",
      " * Video editing (e.g., color correction, motion tracking)\n",
      "3. **Chatbots and Customer Service**: Generative AI can power chatbots that can understand and respond to customer inquiries in a more human-like way.\n",
      "4. **Personalization**: Generative AI can be used to personalize content and experiences for customers based on their preferences, behavior, and demographics.\n",
      "5. **Marketing and Advertising**: Generative AI can be used to generate:\n",
      " * Ad creative (e.g., images, videos, copy)\n",
      " * Social media ads\n",
      " * Email marketing campaigns\n",
      "6. **Data Augmentation**: Generative AI can be used to augment existing data sets, making them more diverse and representative.\n",
      "7. **Predictive Analytics**: Generative AI can be used to generate predictions for business outcomes such as sales forecasts or customer churn probabilities.\n",
      "8. **Influencer Marketing**: Generative AI can be used to generate influencer content (e.g., product showcases, testimonials) that is tailored to specific audiences.\n",
      "9. **Automated Content Writing**: Generative AI can be used to automate content writing tasks for websites, blogs, and other publications.\n",
      "10. **Creative Industries**: Generative AI can be used in creative industries such as music composition, film scoring, or art generation.\n",
      "\n",
      "Some notable companies that are using Generative AI include:\n",
      "\n",
      "1. Google (e.g., Google Lens, Google Assistant)\n",
      "2. Amazon (e.g., Alexa, Amazon SageMaker)\n",
      "3. Facebook (e.g., AI-powered ads, Messenger bots)\n",
      "4. Microsoft (e.g., Azure Cognitive Services, Office 365)\n",
      "5. IBM (e.g., Watson, AI-powered customer service)\n",
      "\n",
      "These are just a few examples of the many business applications of Generative AI. As technology continues to evolve, we can expect to see even more innovative uses of this powerful tool.\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Content Creation**: Generative AI can be used to generate high-quality content such as articles, social media posts, product descriptions, and more. This can help reduce content creation costs and improve efficiency.\n",
      "2. **Marketing Automation**: Generative AI can be used to personalize marketing messages and offers to individual customers, improving engagement and conversion rates.\n",
      "3. **Customer Service**: Generative AI-powered chatbots can be used to provide 24/7 customer support, answering common queries and routing complex issues to human agents.\n",
      "4. **Product Design**: Generative AI can be used to design new products, such as furniture, clothing, or electronics, reducing the need for manual prototyping and testing.\n",
      "5. **Supply Chain Optimization**: Generative AI can be used to analyze supply chain data and identify opportunities for cost savings, inventory optimization, and demand forecasting.\n",
      "6. **Financial Analysis**: Generative AI can be used to analyze financial data and identify trends, patterns, and potential risks, helping businesses make more informed investment decisions.\n",
      "7. **Cybersecurity**: Generative AI can be used to detect and respond to cyber threats in real-time, improving the effectiveness of security systems and reducing the risk of data breaches.\n",
      "8. **Healthcare**: Generative AI can be used to analyze medical images, identify patterns in patient data, and develop personalized treatment plans.\n",
      "9. **Education**: Generative AI can be used to create personalized learning experiences for students, adapting content and pace to individual needs and abilities.\n",
      "10. **Creative Writing**: Generative AI can be used to generate creative writing such as poetry, stories, or even entire scripts.\n",
      "\n",
      "Some specific business use cases include:\n",
      "\n",
      "* **Chatbots**: Companies like IBM, Microsoft, and Amazon are using generative AI-powered chatbots to provide customer support and improve user experience.\n",
      "* **Personalized Marketing**: Retailers like Walmart and Target are using generative AI to personalize marketing messages and offers to individual customers.\n",
      "* **Product Design**: Furniture companies like IKEA and fashion brands like Zara are using generative AI to design new products and reduce prototyping costs.\n",
      "* **Predictive Maintenance**: Companies like Siemens and GE are using generative AI to analyze maintenance data and predict equipment failures, reducing downtime and increasing efficiency.\n",
      "\n",
      "These are just a few examples of the many business applications of Generative AI. As the technology continues to evolve, we can expect to see even more innovative use cases across various industries.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
   "metadata": {},
   "source": [
    "## Are you confused about why that works?\n",
    "\n",
    "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
    "\n",
    "Here's the scoop:\n",
    "\n",
    "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
    "\n",
    "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
    "\n",
    "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
    "\n",
    "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
    "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
    "\n",
    "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
    "\n",
    "That's it!\n",
    "\n",
    "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
    "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "\n",
    "# If you get an error running this cell, then please head over to the troubleshooting notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78a6e110-7be2-4773-acdf-85d9d05afbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ‰ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì Ÿ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡Š \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡‡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì¡ƒ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ² \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ³ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ƒ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì  \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì ‰ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ï¿½ì Ÿ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼… 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼…   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ï¿½ë¼ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë»½ï¿½ë¼…  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d945fada-f486-49e4-8617-2a7c016f3715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "171fd7ed-8451-4724-bb3a-4a1ce3b94142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.  when you summarize, please convert whole message to Korean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4b88389-9b37-4e15-8e92-a8590713e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0191e7b7-29c9-4c84-855c-eda1d0a96d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fc29060-b226-4928-b263-e6bed3540335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b12c44a4-73d0-43a2-b24e-62d9c4b73388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98f7568e-61dd-4d2c-bfcb-d50eb4ecd85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)     # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    response = ollama_via_openai.chat.completions.create(\n",
    "        model = MODEL,\n",
    "        messages = messages_for(website)            #ì»´í”Œë¦¬ì…˜ apiì— ì‹œìŠ¤í…œ/ìœ ì €í”„ë¡¬í”„íŠ¸ ì „ë‹¬\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7018fce0-44c9-4e9c-9e97-614ec1be069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is the summary of the website in Korean:\\n\\nHOME\\n===================================================== \\n\\n* ì—ë“œì›Œë“œ Ø¯ÙˆÙ†ë„ˆ (Edward Donner)ê°€ ìš´ì˜í•˜ëŠ” ãƒ›ãƒ¼ãƒ  íŒŒì§€íŠ¸ì…ë‹ˆë‹¤.\\n* ë¡œ.Botê°€ ìš´ì˜ë˜ëŠ” ê²Œì„ê³¼ AIì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ìì§ˆì„ ì œê³µí•©ë‹ˆë‹¤.\\n\\n**ABOUT**\\n---------\\n\\n### About\\n\\nì´ ì‚¬ëŒì˜ ê¸€ì€ Edward Donnerì…ë‹ˆë‹¤.\\nì—ë“œì›Œë“œëŠ”-codeë¥¼ ì¡°ì •í•˜ê³  LLMsì˜ ì‹¤í—˜ì„ ì›í•˜ë©°hopefully similarlyì˜ ë¶„ì´ ë³´ëŠ” ê³³ì— ìˆìœ¼ì‹  ê²ƒì´ë‹¤. \\nìŒì•… ì œí•œ, DJing ë° electronic music productionì„ ì¢‹ì•„í•©ë‹ˆë‹¤ (but badlyë¡œ practiceë¥¼ í•œë‹¤ëŠ” ë§ì´ ìˆìŠµë‹ˆë‹¤!).\\n\\n### Posts\\n\\n2025ë…„ 5ì›” 28ì¼\\në„ë©”ì¸ connecting my courses â€“ become an LLM expert and leader\\n\\n2025ë…„ 5ì›” 18ì¼\\n2025 AI Executive Briefing\\n\\n2025ë…„ 4ì›” 21ì¼\\nComplete Agentic AI Engineering Course\\n\\n2025ë…„ 1ì›” 23ì¼\\nLLM Workshop- Hands-on with Agent- resources\\n\\n\\n2025ë…„ 4ì›” 21ì¼\\n2025ë…„ ì´í›„ - AI ejecutive briefing\\n\\nì•„ë‹ˆcreasing- become an LLM expert and leader \\n\\nìê²© earned â€“ LLM expert ë° leader (ì½”ãƒ¼ã‚¹)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60836568-34d8-43bb-adc7-3adab650c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)   #ìœ„ì— ëŒë¦°ê°’ ë³€ìˆ˜ì— ë„£ê³ \n",
    "    display(Markdown(summary))   #í‘œì¶œí•¨ìˆ˜ ëŒë¦¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3574846a-a7bf-4f20-ad98-e229ca56fff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summary\n",
       "### Website Overview\n",
       "The website \"Home - Edward Donner\" is a personal blog written by Ed Donner, the co-founder and CTO of Nebula.io. The site focuses on topics related to AI, particularly Large Language Models (LLMs) in the field of talent discovery and management.\n",
       "\n",
       "### Key Articles/Announcements\n",
       "\n",
       "#### Recent Posts\n",
       "\n",
       "*   **Connecting my courses â€“ become an LLM expert and leader**: A post announcing Ed's upcoming online courses where users can learn about LLMs.\n",
       "*   **2025 AI Executive Briefing**: An announcement for a newsletter dedicated to providing insights into the future of AI industry trends.\n",
       "*   **The Complete Agentic AI Engineering Course** and **LLM Workshop â€“ Hands-on with Agents â€“ resources**: Announcements for upcoming online courses that offer hands-on experience with agents in LLMs and provide additional resource materials.\n",
       "\n",
       "#### Archives\n",
       "\n",
       "The website appears to store archives of Ed's past posts, including:\n",
       "\n",
       "*   Posts about DJing and amateur electronic music production\n",
       "*   A reflection about his experiences at Hacker News discussions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6cf1c-0f61-4bb3-a569-7916560bd6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
